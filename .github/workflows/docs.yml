name: Docs

on:
  push:
    branches: [ "main" ]
    paths:
      - "docs/**"
      - "mkdocs.yml"
      - "requirements.txt"
  workflow_dispatch:
  workflow_run:
    workflows:
      - "CaribData Open Data — Build & Release"
      - "CaribData Messy Data — Fetch & Bundle"
      - "CaribData Messy Data — Release"
    types: [completed]

permissions:
  contents: write
  actions: read

concurrency:
  group: docs-site
  cancel-in-progress: true

jobs:
  build-deploy:
    if: github.event_name != 'workflow_run' || (github.event.workflow_run.conclusion == 'success')
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install MkDocs (no repo deps required)
        shell: bash
        run: |
          set -eux
          pip install mkdocs mkdocs-material
          mkdocs --version

      # Pull current gh-pages so we can read /data/... that release workflows published
      - name: Checkout gh-pages (read-only)
        uses: actions/checkout@v4
        continue-on-error: true
        with:
          ref: gh-pages
          path: ghp
          fetch-depth: 0

      - name: Ensure gh-pages workspace exists
        shell: bash
        run: |
          set -eux
          mkdir -p ghp
          mkdir -p ghp/data
          ls -la ghp || true
          ls -la ghp/data || true
          [ -f ghp/data/latest.json ] && echo "latest.json:" && cat ghp/data/latest.json || echo "(no latest.json yet)"

      # Generate docs/downloads.md from files actually present on gh-pages
      - name: Generate Downloads page
        shell: bash
        run: |
          set -euo pipefail
          set -x
          mkdir -p docs

          # BASE_URL for HTTPS links
          OWNER="${GITHUB_REPOSITORY_OWNER}"
          REPO="${GITHUB_REPOSITORY#*/}"
          BASE_URL="https://${OWNER}.github.io/${REPO}"

          urlencode_path () {
            local in="$1"
            in="${in// /%20}"; in="${in//(/%28}"; in="${in//)/%29}"
            in="${in//&/%26}"; in="${in//#/%23}"
            echo "$in"
          }

          # Determine latest Open Data tag
          LATEST_TAG=""
          if [ -f ghp/data/latest.json ]; then
            LATEST_TAG="$(grep -oE '"tag"\s*:\s*"[^"]+"' ghp/data/latest.json | sed -E 's/.*"tag"\s*:\s*"([^"]+)".*/\1/' || true)"
          fi
          if [ -z "$LATEST_TAG" ]; then
            # Fallback: pick highest version-like dir starting with v or od-
            set +o pipefail
            LATEST_TAG="$(find ghp/data -mindepth 1 -maxdepth 1 -type d -printf "%f\n" 2>/dev/null | grep -E '^(v|od-)' | sort -V | tail -n 1 || true)"
            set -o pipefail
          fi

          # Determine latest Messy tag
          MESSY_TAG=""
          if [ -d ghp/data/messy ]; then
            set +o pipefail
            MESSY_TAG="$(ls -1 ghp/data/messy | sort -V | tail -n 1 || true)"
            set -o pipefail
          fi

          {
            echo "# Downloads"
            echo

            # ------- Open Data section -------
            if [ -n "$LATEST_TAG" ] && [ -d "ghp/data/$LATEST_TAG" ]; then
              echo "## Open Data — Latest: \`$LATEST_TAG\`"
              echo
              for p in "_freshness.json" "_quality_report.csv" "_quality_report.json" "world_bank/_dictionary.csv" "world_bank/_manifest.json" "faostat_fbs/_manifest.json"; do
                if [ -f "ghp/data/$LATEST_TAG/$p" ]; then
                  rel="data/$LATEST_TAG/$p"
                  echo "- [$p](${BASE_URL}/$(urlencode_path "$rel"))"
                fi
              done
              echo

              # World Bank per-country CSVs
              if [ -d "ghp/data/$LATEST_TAG/world_bank" ]; then
                echo "### World Bank CSVs"
                set +o pipefail
                for country in $(find "ghp/data/$LATEST_TAG/world_bank" -mindepth 1 -maxdepth 1 -type d -printf "%f\n" 2>/dev/null | sort); do
                  echo "- **$country**"
                  for f in $(find "ghp/data/$LATEST_TAG/world_bank/$country" -type f -name "*.csv" 2>/dev/null | sort); do
                    bn="$(basename "$f")"
                    rel="${f#ghp/}"
                    echo "  - [$bn](${BASE_URL}/$(urlencode_path "$rel"))"
                  done
                done
                set -o pipefail
                echo
              fi

              # FAOSTAT FBS CSVs
              if [ -d "ghp/data/$LATEST_TAG/faostat_fbs" ]; then
                echo "### FAOSTAT FBS CSVs"
                set +o pipefail
                for f in $(find "ghp/data/$LATEST_TAG/faostat_fbs" -type f -name "*_fbs.csv" 2>/dev/null | sort); do
                  bn="$(basename "$f")"
                  rel="${f#ghp/}"
                  echo "- [$bn](${BASE_URL}/$(urlencode_path "$rel"))"
                done
                set -o pipefail
                echo
              fi
            else
              echo "## Open Data — (no published tag found yet)"
              echo
            fi

            # ------- Messy section -------
            echo "## Messy Data (Belize)"
            if [ -n "$MESSY_TAG" ] && [ -d "ghp/data/messy/$MESSY_TAG" ]; then
              echo "_Latest messy tag:_ \`$MESSY_TAG\`"
              echo
              for p in "_manifest.json" "_report.json" "_dataset_card.md"; do
                if [ -f "ghp/data/messy/$MESSY_TAG/$p" ]; then
                  rel="data/messy/$MESSY_TAG/$p"
                  echo "- [$p](${BASE_URL}/$(urlencode_path "$rel"))"
                fi
              done
              echo
              echo "### Raw files"
              if [ -d "ghp/data/messy/$MESSY_TAG/raw" ]; then
                set +o pipefail
                for slug in $(find "ghp/data/messy/$MESSY_TAG/raw" -mindepth 1 -maxdepth 1 -type d -printf "%f\n" 2>/dev/null | sort); do
                  echo "- **$slug**"
                  for f in $(find "ghp/data/messy/$MESSY_TAG/raw/$slug" -type f \( -iname "*.xlsx" -o -iname "*.xls" -o -iname "*.csv" \) 2>/dev/null | sort); do
                    bn="$(basename "$f")"
                    rel="${f#ghp/}"
                    echo "  - [$bn](${BASE_URL}/$(urlencode_path "$rel"))"
                  done
                done
                set -o pipefail
                echo
              fi
            else
              echo "_No messy data published yet._"
              echo
            fi

            # ------- All Open Data tags -------
            echo "## All Open Data tags"
            set +o pipefail
            TAGS="$(find ghp/data -mindepth 1 -maxdepth 1 -type d -printf "%f\n" 2>/dev/null | grep -E '^(v|od-)' | sort -V || true)"
            set -o pipefail
            if [ -n "$TAGS" ]; then
              while IFS= read -r t; do
                [ -z "$t" ] && continue
                echo "- [$t](${BASE_URL}/data/$(urlencode_path "$t")/)"
              done <<< "$TAGS"
              echo
            fi
          } > docs/downloads.md

          echo "DEBUG: first lines of generated docs/downloads.md"
          head -n 120 docs/downloads.md || true

      - name: Build site
        shell: bash
        run: |
          set -eux
          mkdocs build --strict

      - name: Deploy to GitHub Pages (preserve existing data/)
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./site
          publish_branch: gh-pages
          keep_files: true
          force_orphan: false
